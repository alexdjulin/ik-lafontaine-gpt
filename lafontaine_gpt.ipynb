{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build La Fontaine GPT\n",
    "In this notebook we build a GPT from scratch that talks in the style of French author Jean De La Fontaine.\n",
    "\n",
    "Tutorial:   \n",
    "\n",
    "[Let's build GPT: from scratch, in code, spelled out](https://youtu.be/kCc8FmEb1nY)\n",
    "\n",
    "[nanoGPT github repo](https://github.com/karpathy/nanoGPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "SEP = 50 * '-'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "dataset_path = 'dataset/tiny-lafontaine.txt'\n",
    "with open(dataset_path, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  435267\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LA CIGALE ET LA FOURMI\n",
      "La Cigale, ayant chanté\n",
      "Tout L'Été,\n",
      "Se trouva fort dépourvue\n",
      "Quand la Bise fut venue.\n",
      "Pas un seul petit morceau\n",
      "De mouche ou de vermisseau.\n",
      "Elle alla crier famine\n",
      "Chez la Fourmi sa voisine,\n",
      "La priant de lui prêter\n",
      "Quelque grain pour subsister\n",
      "Jusqu'à la saison nouvelle.\n",
      "« Je vous paierai, lui dit-elle,\n",
      "Avant l'Août, foi d'animal,\n",
      "Intérêt et principal. »\n",
      "La Fourmi n'est pas prêteuse :\n",
      "C'est là son moindre défaut.\n",
      "« Que faisiez-vous au temps chaud ?\n",
      "Dit-elle à cette emprunteuse.\n",
      "- Nuit et jour à tout venant\n",
      "Je chantais, ne vous déplaise.\n",
      "- Vous chantiez ? j'en suis fort aise :\n",
      "Eh bien ! dansez maintenant. »\n",
      "LE CORBEAU ET LE RENARD\n",
      "Maître Corbeau, sur un arbre perché,\n",
      "Tenait en son bec un fromage.\n",
      "Maître Renard, par l'odeur alléché,\n",
      "Lui tint à peu près ce langage :\n",
      "« Et bonjour, Monsieur du Corbeau.\n",
      "Que vous êtes joli ! que vous me semblez beau !\n",
      "Sans mentir, si votre ramage\n",
      "Se rapporte à votre plumage,\n",
      "Vous êtes le Phénix des hôtes de ces Bois. »\n",
      "A ces mots le corb\n"
     ]
    }
   ],
   "source": [
    "# let's look at the first 1000 characters\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !\"'(),-.:;?ABCDEFGHIJLMNOPQRSTUVXYZabcdefghijlmnopqrstuvxyz«»ÀÂÇÈÉÊÎÔÛàâçèéêîïôùûœ\n",
      "84 characters\n"
     ]
    }
   ],
   "source": [
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size, 'characters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "Le'ts build a very simple tokenizer and and encoder/decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14, 50, 49, 46, 50, 56, 53]\n",
      "Bonjour\n"
     ]
    }
   ],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = {ch:i for i, ch in enumerate(chars)}  # chars -> ints table\n",
    "itos = {i:ch for i, ch in enumerate(chars)}  # ints -> chars table\n",
    "encode = lambda s: [stoi[c] for c in s]  # encoder: takes a string, outputs a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l])  # decoder: takes a list of integers, output a string\n",
    "\n",
    "print(encode('Bonjour'))\n",
    "print(decode(encode('Bonjour')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([435267]) torch.int64\n",
      "tensor([23, 13,  1, 15, 21, 19, 13, 23, 17,  1, 17, 31,  1, 23, 13,  1, 18, 26,\n",
      "        32, 29, 24, 21,  0, 23, 37,  1, 15, 45, 43, 37, 47, 41,  7,  1, 37, 59,\n",
      "        37, 49, 55,  1, 39, 44, 37, 49, 55, 76,  0, 31, 50, 56, 55,  1, 23,  4,\n",
      "        67, 55, 76,  7,  0, 30, 41,  1, 55, 53, 50, 56, 57, 37,  1, 42, 50, 53,\n",
      "        55,  1, 40, 76, 51, 50, 56, 53, 57, 56, 41,  0, 28, 56, 37, 49, 40,  1,\n",
      "        47, 37,  1, 14, 45, 54, 41,  1, 42, 56])\n"
     ]
    }
   ],
   "source": [
    "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([391740]) torch.Size([43527])\n"
     ]
    }
   ],
   "source": [
    "# Now let's split up the data into train and validation sets\n",
    "n = int(0.9 * len(data))   # first 90% of the data will be the training set, rest will be the validation set\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "print(train_data.shape, val_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block Size and Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([23, 13,  1, 15, 21, 19, 13, 23, 17])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a blocksize (context window). The model will be trained to predict the next character \n",
    "# given a block of characters of this size\n",
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When input is tensor([23]) the target is: 13\n",
      "When input is tensor([23, 13]) the target is: 1\n",
      "When input is tensor([23, 13,  1]) the target is: 15\n",
      "When input is tensor([23, 13,  1, 15]) the target is: 21\n",
      "When input is tensor([23, 13,  1, 15, 21]) the target is: 19\n",
      "When input is tensor([23, 13,  1, 15, 21, 19]) the target is: 13\n",
      "When input is tensor([23, 13,  1, 15, 21, 19, 13]) the target is: 23\n",
      "When input is tensor([23, 13,  1, 15, 21, 19, 13, 23]) the target is: 17\n"
     ]
    }
   ],
   "source": [
    "# visualise input context window and target\n",
    "# we are sampling a context window from as little as 1 to the complete block size, as we want\n",
    "# the model to later be able to generate text from any context window size\n",
    "\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"When input is {context} the target is: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[ 1, 37, 56,  1, 44, 37, 54, 37],\n",
      "        [39, 44, 37,  1, 52, 56, 41, 47],\n",
      "        [41,  1, 47, 37,  1, 53, 37, 45],\n",
      "        [ 1, 37, 57, 37, 45, 55,  1, 51]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[37, 56,  1, 44, 37, 54, 37, 53],\n",
      "        [44, 37,  1, 52, 56, 41, 47, 52],\n",
      "        [ 1, 47, 37,  1, 53, 37, 45, 54],\n",
      "        [37, 57, 37, 45, 55,  1, 51, 41]])\n",
      "--------------------------------------------------\n",
      "When input is [1] the target is: 37\n",
      "When input is [1, 37] the target is: 56\n",
      "When input is [1, 37, 56] the target is: 1\n",
      "When input is [1, 37, 56, 1] the target is: 44\n",
      "When input is [1, 37, 56, 1, 44] the target is: 37\n",
      "When input is [1, 37, 56, 1, 44, 37] the target is: 54\n",
      "When input is [1, 37, 56, 1, 44, 37, 54] the target is: 37\n",
      "When input is [1, 37, 56, 1, 44, 37, 54, 37] the target is: 53\n",
      "When input is [39] the target is: 44\n",
      "When input is [39, 44] the target is: 37\n",
      "When input is [39, 44, 37] the target is: 1\n",
      "When input is [39, 44, 37, 1] the target is: 52\n",
      "When input is [39, 44, 37, 1, 52] the target is: 56\n",
      "When input is [39, 44, 37, 1, 52, 56] the target is: 41\n",
      "When input is [39, 44, 37, 1, 52, 56, 41] the target is: 47\n",
      "When input is [39, 44, 37, 1, 52, 56, 41, 47] the target is: 52\n",
      "When input is [41] the target is: 1\n",
      "When input is [41, 1] the target is: 47\n",
      "When input is [41, 1, 47] the target is: 37\n",
      "When input is [41, 1, 47, 37] the target is: 1\n",
      "When input is [41, 1, 47, 37, 1] the target is: 53\n",
      "When input is [41, 1, 47, 37, 1, 53] the target is: 37\n",
      "When input is [41, 1, 47, 37, 1, 53, 37] the target is: 45\n",
      "When input is [41, 1, 47, 37, 1, 53, 37, 45] the target is: 54\n",
      "When input is [1] the target is: 37\n",
      "When input is [1, 37] the target is: 57\n",
      "When input is [1, 37, 57] the target is: 37\n",
      "When input is [1, 37, 57, 37] the target is: 45\n",
      "When input is [1, 37, 57, 37, 45] the target is: 55\n",
      "When input is [1, 37, 57, 37, 45, 55] the target is: 1\n",
      "When input is [1, 37, 57, 37, 45, 55, 1] the target is: 51\n",
      "When input is [1, 37, 57, 37, 45, 55, 1, 51] the target is: 41\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)  # for reproducibility\n",
    "batch_size = 4  # how many independent sequences will we process in parallel\n",
    "block_size = 8  # what i sthe maximum context length for predictions\n",
    "\n",
    "def get_batch(split: str) -> [torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Generate a small batch of data of inputs x and targets y\n",
    "\n",
    "    Args:\n",
    "        split (str): dataset split to sample from ('train' or 'val')\n",
    "\n",
    "    Returns:\n",
    "        x (torch.Tensor): input data\n",
    "        y (torch.Tensor): target data\n",
    "    \"\"\"\n",
    "\n",
    "    data = train_data if split == 'train' else val_data  # choose the split\n",
    "\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))  # sample random starting indices for the sequences\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])  # create a batch of context windows\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])  # create a batch of targets, one step forward\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print(SEP)\n",
    "\n",
    "for b in range(batch_size):  # batch dimension\n",
    "    for t in range(block_size):  # time dimension\n",
    "        context = xb[b, :t+1]  # select the context window\n",
    "        target = yb[b, t]  # select the target\n",
    "        print(f\"When input is {context.tolist()} the target is: {target}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1, 37, 56,  1, 44, 37, 54, 37],\n",
      "        [39, 44, 37,  1, 52, 56, 41, 47],\n",
      "        [41,  1, 47, 37,  1, 53, 37, 45],\n",
      "        [ 1, 37, 57, 37, 45, 55,  1, 51]])\n"
     ]
    }
   ],
   "source": [
    "# our input to the transformer\n",
    "print(xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 84])\n",
      "tensor(5.1988, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "'O)'(QçFJyXS'x\n",
      "fVVZyrébbéY\n",
      "SEcOpAgPcZpç?Rj,p\"ÇùfsSghêÔy«R!ÈeêqCÈYzJY(«SbéphXB!DFgjrZÇùÊpDttHdlFlOSuj\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)  # for reproducibility\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits from the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx and targets are both (B, T) tensors of integers\n",
    "        logits = self.token_embedding_table(idx)  # (B, T, C) = Batch, Time (block_size), Channels (vocab_size)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "\n",
    "        else:\n",
    "            # reshape the logits to be (B*T, C) and the targets to be (B*T) so we can compute the loss\n",
    "            B, T, C = logits.shape  # unpack batch, time, channels\n",
    "            logits = logits.view(B*T, C)  # flatten the Time and Batch dimensions\n",
    "            targets = targets.view(B*T)\n",
    "            \n",
    "            # compute the loss using cross entropy = quality of the logicts in respect to the targets\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is a (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)  # (B, T, C)  internally calls the forward method in pytorch\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :]  # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
    "        \n",
    "        return idx\n",
    "    \n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)  # calling the model and passing in the input and the targets\n",
    "print(logits.shape)  # should be (B, T, C) \n",
    "print(loss)  # loss should be close to -ln(1/vocab_size)\n",
    "\n",
    "idx_0 = torch.zeros((1, 1), dtype=torch.long) # initial context is just a single 0\n",
    "print(decode(m.generate(idx=idx_0, max_new_tokens = 100)[0].tolist()))  # generate 100 new tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** right now the history is not used. The next character is predicted only using the previous one and not the full sequence. This result is very random right now. We want to train the model so it becomes less random."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)  # AdamW is a good optimizer for transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.426698684692383\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "for steps in range(10000):\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)  # calling the model and passing in the input and the targets\n",
    "    optimizer.zero_grad(set_to_none=True)  # clear previous gradients\n",
    "    loss.backward()  # compute new gradients\n",
    "    optimizer.step()  # update the weights\n",
    "\n",
    "print(loss.item())  # print our training loss value at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Maral HEt Ce ngeie\n",
      "Pont t me,\n",
      "Lent aret s, pars cen mét Turt me.\n",
      "MA mben dége lar inde bonogorious jus.\n",
      "Coue boreré fa fûre éne vi le s aunivrtagaite. Pe aceffaitr\n",
      "Lerrmeneusourôtois ;\n",
      "Jetrtrt vra e.\n",
      "OIIll?\n",
      "( lempe l lares d'eurs emblare ntôteu uves ;\n",
      "NAj'e cèr sispailueux jest pits t vrre ce fometitasevoin\n",
      "Dirimi de.\n",
      "Qus de d, l saitea né nz phe mouaitt sen curcogr joivan à sosanend'autit lsomainen ces t mourit s s vouns.\n",
      "Noue oilan ;\n",
      "Pafrennesastrte.\n",
      "Chun rortis,\n",
      "DEnt ait rs llex lene ciléne d\n"
     ]
    }
   ],
   "source": [
    "idx_0 = torch.zeros((1, 1), dtype=torch.long)\n",
    "print(decode(m.generate(idx=idx_0, max_new_tokens = 500)[0].tolist()))  # generate 100 new tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The mathematical trick in self attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "--------------------------------------------------\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--------------------------------------------------\n",
      "c=\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "# version 1\n",
    "torch.manual_seed(42)  # for reproducibility\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "c = a @ b\n",
    "\n",
    "print('a=')\n",
    "print(a)\n",
    "print(SEP)\n",
    "print('b=')\n",
    "print(b)\n",
    "print(SEP)\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# consider the following toy example:\n",
    "torch.manual_seed(1337)  # for reproducibility\n",
    "B, T, C = 4, 8, 2  # batch size, time steps, channels\n",
    "x = torch.randn(B, T, C)  # random input data\n",
    "x.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.3596, -0.9152],\n",
       "        [ 0.6258,  0.0255],\n",
       "        [ 0.9545,  0.0643],\n",
       "        [ 0.3612,  1.1679],\n",
       "        [-1.3499, -0.5102],\n",
       "        [ 0.2360, -0.2398],\n",
       "        [-0.9211,  1.5433]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a bag of words tensor\n",
    "# we want x[b, t] = mean_{i<=t} x[b, i]\n",
    "xbow = torch.zeros((B, T, C))  # output tensor initialized at zero\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1]  # (t, C)\n",
    "        xbow[b, t] = torch.mean(xprev, 0)  # (C)\n",
    "\n",
    "x[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 2\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "xbow2 = wei @ x  # (T, T) @ (B, T, C) = (B, T, C)\n",
    "torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.1808, -0.0700],\n",
       "         [-0.0894, -0.4926],\n",
       "         [ 0.1490, -0.3199],\n",
       "         [ 0.3504, -0.2238],\n",
       "         [ 0.3525,  0.0545],\n",
       "         [ 0.0688, -0.0396],\n",
       "         [ 0.0927, -0.0682],\n",
       "         [-0.0341,  0.1332]]),\n",
       " tensor([[ 0.1808, -0.0700],\n",
       "         [-0.0894, -0.4926],\n",
       "         [ 0.1490, -0.3199],\n",
       "         [ 0.3504, -0.2238],\n",
       "         [ 0.3525,  0.0545],\n",
       "         [ 0.0688, -0.0396],\n",
       "         [ 0.0927, -0.0682],\n",
       "         [-0.0341,  0.1332]]))"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0], xbow2[0]  # should be identical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tril\n",
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "--------------------------------------------------\n",
      "wei init\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "--------------------------------------------------\n",
      "wei masked\n",
      "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "--------------------------------------------------\n",
      "wei softmax\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 3: use softmax\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "print('tril')\n",
    "print(tril)\n",
    "print(SEP)\n",
    "\n",
    "wei = torch.zeros((T, T))\n",
    "print('wei init')\n",
    "print(wei)\n",
    "print(SEP)\n",
    "\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))  # all elements where triu is 0 are set to -inf for softmax\n",
    "print('wei masked')\n",
    "print(wei)\n",
    "print(SEP)\n",
    "\n",
    "wei = F.softmax(wei, dim=1)\n",
    "print('wei softmax')\n",
    "print(wei)\n",
    "print(SEP)\n",
    "\n",
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow, xbow3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 4: self attention!\n",
    "torch.manual_seed(1337)  # for reproducibility\n",
    "B, T, C = 4, 8, 32  # batch size, time steps, channels\n",
    "x = torch.randn(B, T, C)  # random input data\n",
    "\n",
    "# let's see a single Head perform self-attention\n",
    "head_size = 16  # smaller head size\n",
    "key = nn.Linear(C, head_size, bias=False)  # key projection\n",
    "query = nn.Linear(C, head_size, bias=False)  # query projection\n",
    "value = nn.Linear(C, head_size, bias=False)  # value projection\n",
    "k = key(x)  # (B, T, head_size)\n",
    "q = query(x)  # (B, T, head_size)\n",
    "wei = q @ k.transpose(-2, -1)  # (B, T, head_size) @ (B, head_size, T) --> (B, T, T) \n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))  # lower triangular matrix\n",
    "# wei = torch.zeros((T, T))  # attention weights, dot product between key (what am I looking for) and query (what do I contain)\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))  # set the upper triangle to -inf\n",
    "wei = F.softmax(wei, dim=-1)  # apply softmax to get the weights\n",
    "\n",
    "v = value(x)  # (B, T, head_size)\n",
    "# out = wei @ x\n",
    "out = wei @ v  # (B, T, T) @ (B, T, head_size) --> (B, T, head_size)\n",
    "\n",
    "print(out.shape)  # should be (B, T, C)\n",
    "wei[0]\n",
    "\n",
    "# this is showing us how much information to aggregate from any of each token in the past"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:**\n",
    "- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
    "\n",
    "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
    "\n",
    "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other.\n",
    "\n",
    "- In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
    "\n",
    "- \"Self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention,\" the queries still get produced from x, but the keys and values come from some other, external source (e.g., an encoder module).\n",
    "\n",
    "- \"Scaled\" attention additionally divides `wei` by 1/sqrt(head_size). This makes it so when input Q, K are unit variance, `wei` will be unit variance too and Softmax will stay diffuse and not saturate too much.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = torch.randn(B, T, head_size)  # key\n",
    "q = torch.randn(B, T, head_size)  # query\n",
    "wei = q @ k.transpose(-2, -1) * head_size**-0.5  # multiply by 1/sqrt(head_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9006)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0037)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.var() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9957)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multiply by 8, softmax sharpens the distribution to avoid a peaky distribution\n",
    "# the scaling is used to control the variance at initialization\n",
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]) * 8, dim=-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ironhack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
