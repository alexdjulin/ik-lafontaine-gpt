{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement tensorflow-addons (from versions: none)\n",
      "ERROR: No matching distribution found for tensorflow-addons\n"
     ]
    }
   ],
   "source": [
    "!pip install -q huggingface_hub\n",
    "!pip install -q onnx\n",
    "!pip install -q onnx-tf\n",
    "!pip install -q tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2764c28a70f42c0a54f42af799e59d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# login to HF\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model and run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from models/lafontaine_gpt_v8_241011_1307.pth.\n",
      "\n",
      "Pallus qu'en des sacrifices de Maximères.\n",
      "Le Dieu les escadrons pour les milles est venus,\n",
      "On se font pris amuser tous les jours déplaire.\n",
      "Philémon les pauvret Baudets\n",
      "Qu'ils n'approchaient leur faire :\n",
      "Car cultait de boire ; ils s'enfuirent gagner.\n",
      "Je vous plains, je te promis, j'en serai par plus monarque des hommes,\n",
      "De femme, ayent de humains, qu'un Maître de l'air,\n",
      "Du restentier et des morts sa main querelle.\n",
      "Le sage qui les mit sont d'en prince le croire ;\n",
      "Et, s'il est aurait venir\n",
      "De ce qu\n"
     ]
    }
   ],
   "source": [
    "import v8_bigram_scaled as v8\n",
    "\n",
    "model_path = 'models/lafontaine_gpt_v8_241011_1307.pth'\n",
    "model = v8.load_model(model_path)\n",
    "\n",
    "text = v8.run_inference(model)\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert model to onnx (deprecated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from models/lafontaine_gpt_v8_241011_1307.pth.\n",
      "Model exported to models/lafontaine_gpt_v8_241011_1307.onnx.\n"
     ]
    }
   ],
   "source": [
    "import v8_bigram_scaled as v8\n",
    "\n",
    "model_path = 'models/lafontaine_gpt_v8_241011_1307.pth'\n",
    "model = v8.load_model(model_path)\n",
    "\n",
    "onnx_model_path = model_path.replace('.pth', '.onnx')\n",
    "v8.export_onnx_model(model, onnx_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run inference in Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nath\\AppData\\Local\\Temp\\ipykernel_18284\\3579305608.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=self.device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "from v8_bigram_scaled import BigramLanguageModel, encode, decode\n",
    "\n",
    "# Assuming 'BigramLanguageModel' and 'decode' are defined as in your code\n",
    "\n",
    "class GradioInterface:\n",
    "    def __init__(self, model_path=None):\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.model = self.load_model(model_path)\n",
    "        self.model.eval()\n",
    "\n",
    "    def load_model(self, model_path):\n",
    "        model = BigramLanguageModel().to(self.device)\n",
    "        if model_path:\n",
    "            model.load_state_dict(torch.load(model_path, map_location=self.device))\n",
    "        return model\n",
    "\n",
    "    def generate_text(self, input_text, max_tokens=100):\n",
    "        context = torch.tensor([encode(input_text)], dtype=torch.long, device=self.device)\n",
    "        output = self.model.generate(context, max_new_tokens=max_tokens)\n",
    "        return decode(output[0].tolist())\n",
    "\n",
    "# Load the model\n",
    "model_path = \"models/lafontaine_gpt_v8_241011_1307.pth\"\n",
    "model_interface = GradioInterface(model_path)\n",
    "\n",
    "# Define Gradio interface\n",
    "gr_interface = gr.Interface(\n",
    "    fn=model_interface.generate_text,\n",
    "    inputs=[\"text\", gr.Slider(50, 500)],\n",
    "    outputs=\"text\",\n",
    "    description=\"Bigram Language Model text generation. Enter some text, and the model will continue it.\",\n",
    "    examples=[[\"Once upon a time\"]]\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "gr_interface.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Push model to hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/alexdjulin/lafontaine-gpt into local empty directory.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0cf31c3c10c433887c925f910a48226",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Download file model.safetensors:   0%|          | 25.5k/42.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import Repository\n",
    "\n",
    "# clone the repository\n",
    "repo = Repository(local_dir=\"deployment/v1\", clone_from=\"https://huggingface.co/alexdjulin/lafontaine-gpt\", use_auth_token=os.environ[\"HF_TOKEN\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import Repository\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "repo = Repository(\n",
    "    local_dir=\"deployment/v1\",\n",
    "    clone_from=\"https://huggingface.co/alexdjulin/lafontaine-gpt\",\n",
    "    use_auth_token=os.environ.get(\"HF_TOKEN\")\n",
    ")\n",
    "repo.push_to_hub()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ironhack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
